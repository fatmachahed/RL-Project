# Reinforcement Learning Project - A2C on CartPole-v1

## üìå Overview

This project implements **Advantage Actor-Critic (A2C)** agents with various configurations to explore the impact of:
- **Parallel workers (K)**: Multiple environments running simultaneously
- **n-step returns**: Multi-step bootstrapping for gradient estimation
- **Stochastic rewards**: Reward masking to simulate real-world uncertainty

The agents are trained on the **CartPole-v1** environment from Gymnasium, with comprehensive performance analysis and visualizations.

---

## üéØ Project Goals

1. Implement a correct A2C algorithm following academic specifications
2. Compare 5 different agent configurations
3. Analyze the effects of parallel workers and n-step returns
4. Study learning stability under stochastic reward conditions
5. Generate publication-ready visualizations

---

## üóÇ Project Structure
```
RL-Project/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ model.py           # ActorCritic neural network
‚îÇ   ‚îú‚îÄ‚îÄ a2c_agent.py       # A2C agent with n-step returns
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py        # Evaluation and trajectory collection
‚îÇ   ‚îî‚îÄ‚îÄ train.py           # Main training loop
‚îÇ
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ agent0.yaml        # K=1, n=1, deterministic
‚îÇ   ‚îú‚îÄ‚îÄ agent1.yaml        # K=1, n=1, stochastic
‚îÇ   ‚îú‚îÄ‚îÄ agent2.yaml        # K=6, n=1, stochastic
‚îÇ   ‚îú‚îÄ‚îÄ agent3.yaml        # K=1, n=6, stochastic
‚îÇ   ‚îî‚îÄ‚îÄ agent4.yaml        # K=6, n=6, stochastic
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ run_all_agents.py  # Train all agents
‚îÇ   ‚îî‚îÄ‚îÄ test_graphs.py     # Generate visualizations
‚îÇ
‚îú‚îÄ‚îÄ results/               # Training metrics (NPZ files)
‚îú‚îÄ‚îÄ plots_0001/           # plots with  lr_critic: 0.001 
‚îú‚îÄ‚îÄ plots_0003/           # plots with lr_critic: 0.003
‚îú‚îÄ‚îÄ notebooks/            # Analysis notebooks
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Installation

### 1Ô∏è‚É£ Create a virtual environment
```bash
python -m venv venv
```

**Activate it:**
- Windows: `venv\Scripts\activate`
- Linux/Mac: `source venv/bin/activate`

### 2Ô∏è‚É£ Install dependencies
```bash
pip install -r requirements.txt
```

**Required libraries:**
- `torch` >= 2.0.0
- `gymnasium` >= 0.29.0
- `numpy` >= 1.24.0
- `matplotlib` >= 3.7.0
- `seaborn` >= 0.12.0
- `pyyaml` >= 6.0
- `tqdm` >= 4.65.0

---

## üöÄ Usage

### Train all agents (3 seeds each)
```bash
python -m tests.run_all_agents
```

This will train 5 agents √ó 3 seeds = 15 runs and save metrics in `results/`.

### Generate visualizations
```bash
python -m tests.test_graphs
```

All plots will be saved in `plots_0001/` and `plots_0003/`  with timestamps.

### Train a single agent
```bash
python -m src.train --config configs/agent0.yaml
```

---

## ü§ñ Agent Configurations

| Agent   | K (workers) | n (steps) | Stochastic | lr_actor | lr_critic | Description |
|---------|-------------|-----------|------------|----------|-----------|-------------|
| **Agent0** | 1 | 1 | ‚ùå | 1e-5 | 1e-3 | Baseline (deterministic) |
| **Agent1** | 1 | 1 | ‚úÖ | 1e-5 | 1e-3 | Stochastic rewards (90% masking) |
| **Agent2** | 6 | 1 | ‚úÖ | 1e-5 | 1e-3 | Parallel workers |
| **Agent3** | 1 | 6 | ‚úÖ | 1e-5 | 1e-3 | Multi-step returns |
| **Agent4** | 6 | 6 | ‚úÖ | 3e-4 | 3e-3 | Combined (best performance) |

### Common hyperparameters:
- **Environment**: CartPole-v1
- **Max steps**: 500,000
- **Discount factor (Œ≥)**: 0.99
- **Entropy coefficient**: 0.01
- **Evaluation frequency**: 20,000 steps
- **Hidden layers**: 2 √ó 64 neurons (Tanh activation)
- **Optimizer**: Adam (separate LR for actor/critic)

---

## üß† Algorithm Details

### Network Architecture
```
Input (4D state) 
    ‚Üì
Linear(4 ‚Üí 64) + Tanh
    ‚Üì
Linear(64 ‚Üí 64) + Tanh
    ‚Üì
    ‚îú‚îÄ‚Üí Actor: Linear(64 ‚Üí 2)   [action logits]
    ‚îî‚îÄ‚Üí Critic: Linear(64 ‚Üí 1)  [state value]
```

### A2C Update Rule
```
Advantage: A(s,a) = R - V(s)
Actor Loss: -log œÄ(a|s) √ó A(s,a)
Critic Loss: 0.5 √ó (R - V(s))¬≤
Entropy: -Œ£ œÄ(a|s) log œÄ(a|s)

Total Loss = Actor Loss + Critic Loss - entropy_coef √ó Entropy
```

### Key Implementation Features

‚úÖ **Correct bootstrapping**: Distinguishes truncation vs. termination  
‚úÖ **n-step returns**: Bootstraps after n steps for variance reduction  
‚úÖ **Parallel workers**: K synchronized environments for stable gradients  
‚úÖ **Stochastic rewards**: 90% masking (agents 1-4) to simulate uncertainty  
‚úÖ **Gradient clipping**: Max norm = 0.5 for training stability  

---

## üìä Generated Visualizations

The project produces **11 comprehensive plots**:

1. **Learning Curves** - Evaluation rewards (mean ¬± std over 3 seeds)
2. **Training Rewards** - Episode returns during training
3. **Actor Loss** - Policy gradient loss evolution
4. **Critic Loss** - Value function MSE evolution
5. **Value Function Mean** - Average predicted values
6. **Trajectory Values** - V(s) along full episodes
7. **Entropy** - Policy entropy over training
8. **Performance Comparison** - Boxplots of final rewards
9. **Training Stability** - Coefficient of variation analysis
10. **Correlation Heatmap** - Metric relationships
11. **Convergence Speed** - Steps to reach target performance

All plots include:
- Mean curves with ¬±1 std shaded areas (across 3 seeds)
- Smooth curves (moving average for clarity)
- Publication-ready quality (high DPI, clear labels)

---

## üìà Expected Results

### Agent Performance Ranking (best ‚Üí worst):
```
Agent4 (K=6, n=6) > Agent2 (K=6) ‚âà Agent3 (n=6) > Agent0 > Agent1
```

### Key Findings:

- **Agent0**: Should reach 500 reward (optimal policy)
- **Agent1**: Slower/unstable due to stochastic rewards
- **Agent2**: Faster convergence (parallel workers reduce variance)
- **Agent3**: More stable (n-step returns smooth gradients)
- **Agent4**: Best of both worlds (fastest + most stable)

### Value Function Convergence:

With correct bootstrapping:
- **Deterministic**: V(s) ‚âà 495 at optimal policy
- **Stochastic (90% mask)**: V(s) ‚âà 50 at optimal policy

---

## üî¨ Experimental Protocol

### Training:
- 500,000 environment steps per agent
- Log metrics every 1,000 steps
- Evaluate every 20,000 steps
- 3 random seeds per configuration

### Evaluation:
- 10 episodes per evaluation
- Greedy policy (argmax action)
- Fresh environment (no training contamination)
- Full trajectory storage at 100k, 200k, 300k, 400k, 500k steps

---

## üõ† Troubleshooting

### Issue: Agent0 doesn't learn (reward ~10)

**Solution**: Learning rate too low. Try `lr_actor: 3e-4` instead of `1e-5`.

### Issue: High variance in training

**Solution**: Increase K (workers) or n (steps) to reduce gradient variance.



## üë• Team

**Author**: Fatma Chahed  and Dhif Aziz 

**Program**: Business Intelligence & Data Science  
**Institution**: Universit√© Paris Dauphine (Tunis)  
**Course**: Reinforcement Learning (S.Moalla)  
**Date**: February 2026

---
**‚≠ê If you find this project useful, please consider starring it!**
